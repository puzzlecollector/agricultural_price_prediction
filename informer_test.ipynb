{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test The Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import random \n",
    "import os \n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>배추_거래량(kg)</th>\n",
       "      <th>배추_가격(원/kg)</th>\n",
       "      <th>무_거래량(kg)</th>\n",
       "      <th>무_가격(원/kg)</th>\n",
       "      <th>양파_거래량(kg)</th>\n",
       "      <th>양파_가격(원/kg)</th>\n",
       "      <th>건고추_거래량(kg)</th>\n",
       "      <th>건고추_가격(원/kg)</th>\n",
       "      <th>마늘_거래량(kg)</th>\n",
       "      <th>마늘_가격(원/kg)</th>\n",
       "      <th>...</th>\n",
       "      <th>청상추_거래량(kg)</th>\n",
       "      <th>청상추_가격(원/kg)</th>\n",
       "      <th>백다다기_거래량(kg)</th>\n",
       "      <th>백다다기_가격(원/kg)</th>\n",
       "      <th>애호박_거래량(kg)</th>\n",
       "      <th>애호박_가격(원/kg)</th>\n",
       "      <th>캠벨얼리_거래량(kg)</th>\n",
       "      <th>캠벨얼리_가격(원/kg)</th>\n",
       "      <th>샤인마스캇_거래량(kg)</th>\n",
       "      <th>샤인마스캇_가격(원/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80860.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>80272.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>122787.5</td>\n",
       "      <td>1281.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>15019.0</td>\n",
       "      <td>5475.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5125.0</td>\n",
       "      <td>9235.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>2109.0</td>\n",
       "      <td>19159.0</td>\n",
       "      <td>2414.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1422742.5</td>\n",
       "      <td>478.0</td>\n",
       "      <td>1699653.7</td>\n",
       "      <td>382.0</td>\n",
       "      <td>2315079.0</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>699.0</td>\n",
       "      <td>4464.0</td>\n",
       "      <td>141638.0</td>\n",
       "      <td>5210.0</td>\n",
       "      <td>...</td>\n",
       "      <td>38525.5</td>\n",
       "      <td>7631.0</td>\n",
       "      <td>500702.0</td>\n",
       "      <td>2046.0</td>\n",
       "      <td>620539.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>2703.8</td>\n",
       "      <td>3885.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1167241.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>1423482.3</td>\n",
       "      <td>422.0</td>\n",
       "      <td>2092960.1</td>\n",
       "      <td>1213.0</td>\n",
       "      <td>1112.6</td>\n",
       "      <td>4342.0</td>\n",
       "      <td>126207.8</td>\n",
       "      <td>5387.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32615.0</td>\n",
       "      <td>6926.0</td>\n",
       "      <td>147638.0</td>\n",
       "      <td>2268.0</td>\n",
       "      <td>231958.0</td>\n",
       "      <td>2178.0</td>\n",
       "      <td>8810.0</td>\n",
       "      <td>2853.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>1856965.0</td>\n",
       "      <td>1839.0</td>\n",
       "      <td>2055640.0</td>\n",
       "      <td>990.0</td>\n",
       "      <td>2281429.2</td>\n",
       "      <td>990.0</td>\n",
       "      <td>2818.4</td>\n",
       "      <td>19101.0</td>\n",
       "      <td>134359.9</td>\n",
       "      <td>4775.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50730.0</td>\n",
       "      <td>4509.0</td>\n",
       "      <td>282212.3</td>\n",
       "      <td>3001.0</td>\n",
       "      <td>313139.7</td>\n",
       "      <td>3426.0</td>\n",
       "      <td>504242.6</td>\n",
       "      <td>3620.0</td>\n",
       "      <td>283196.9</td>\n",
       "      <td>10940.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>1880095.5</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>1879261.0</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>2074513.0</td>\n",
       "      <td>955.0</td>\n",
       "      <td>1887.1</td>\n",
       "      <td>23095.0</td>\n",
       "      <td>126926.0</td>\n",
       "      <td>5039.0</td>\n",
       "      <td>...</td>\n",
       "      <td>54322.0</td>\n",
       "      <td>4178.0</td>\n",
       "      <td>312214.8</td>\n",
       "      <td>2999.0</td>\n",
       "      <td>362741.0</td>\n",
       "      <td>3357.0</td>\n",
       "      <td>479683.1</td>\n",
       "      <td>3618.0</td>\n",
       "      <td>303779.6</td>\n",
       "      <td>10844.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>1661090.9</td>\n",
       "      <td>1760.0</td>\n",
       "      <td>1709385.7</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>2089081.2</td>\n",
       "      <td>961.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>22510.0</td>\n",
       "      <td>110357.7</td>\n",
       "      <td>5565.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61213.0</td>\n",
       "      <td>3770.0</td>\n",
       "      <td>327395.8</td>\n",
       "      <td>3065.0</td>\n",
       "      <td>390361.2</td>\n",
       "      <td>3092.0</td>\n",
       "      <td>521493.8</td>\n",
       "      <td>3691.0</td>\n",
       "      <td>313295.7</td>\n",
       "      <td>10636.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>25396.0</td>\n",
       "      <td>3066.0</td>\n",
       "      <td>38222.0</td>\n",
       "      <td>1139.0</td>\n",
       "      <td>18240.0</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>22333.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>5227.0</td>\n",
       "      <td>...</td>\n",
       "      <td>144.0</td>\n",
       "      <td>4076.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>3707.0</td>\n",
       "      <td>2464.0</td>\n",
       "      <td>3252.0</td>\n",
       "      <td>21717.0</td>\n",
       "      <td>3567.0</td>\n",
       "      <td>9734.0</td>\n",
       "      <td>10699.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>2405051.9</td>\n",
       "      <td>1867.0</td>\n",
       "      <td>2747519.5</td>\n",
       "      <td>1147.0</td>\n",
       "      <td>2235784.7</td>\n",
       "      <td>964.0</td>\n",
       "      <td>1630.6</td>\n",
       "      <td>22022.0</td>\n",
       "      <td>175584.1</td>\n",
       "      <td>4757.0</td>\n",
       "      <td>...</td>\n",
       "      <td>84155.0</td>\n",
       "      <td>4167.0</td>\n",
       "      <td>554862.6</td>\n",
       "      <td>2873.0</td>\n",
       "      <td>667745.0</td>\n",
       "      <td>2782.0</td>\n",
       "      <td>601841.0</td>\n",
       "      <td>3761.0</td>\n",
       "      <td>382263.4</td>\n",
       "      <td>10998.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1733 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      배추_거래량(kg)  배추_가격(원/kg)  무_거래량(kg)  무_가격(원/kg)  양파_거래량(kg)  양파_가격(원/kg)  \\\n",
       "0            0.0          0.0        0.0         0.0         0.0          0.0   \n",
       "1        80860.0        329.0    80272.0       360.0    122787.5       1281.0   \n",
       "2            0.0          0.0        0.0         0.0         0.0          0.0   \n",
       "3      1422742.5        478.0  1699653.7       382.0   2315079.0       1235.0   \n",
       "4      1167241.0        442.0  1423482.3       422.0   2092960.1       1213.0   \n",
       "...          ...          ...        ...         ...         ...          ...   \n",
       "1728   1856965.0       1839.0  2055640.0       990.0   2281429.2        990.0   \n",
       "1729   1880095.5       1789.0  1879261.0      1011.0   2074513.0        955.0   \n",
       "1730   1661090.9       1760.0  1709385.7      1075.0   2089081.2        961.0   \n",
       "1731     25396.0       3066.0    38222.0      1139.0     18240.0       1056.0   \n",
       "1732   2405051.9       1867.0  2747519.5      1147.0   2235784.7        964.0   \n",
       "\n",
       "      건고추_거래량(kg)  건고추_가격(원/kg)  마늘_거래량(kg)  마늘_가격(원/kg)  ...  청상추_거래량(kg)  \\\n",
       "0             0.0           0.0         0.0          0.0  ...          0.0   \n",
       "1             3.0       11000.0     15019.0       5475.0  ...       5125.0   \n",
       "2             0.0           0.0         0.0          0.0  ...          0.0   \n",
       "3           699.0        4464.0    141638.0       5210.0  ...      38525.5   \n",
       "4          1112.6        4342.0    126207.8       5387.0  ...      32615.0   \n",
       "...           ...           ...         ...          ...  ...          ...   \n",
       "1728       2818.4       19101.0    134359.9       4775.0  ...      50730.0   \n",
       "1729       1887.1       23095.0    126926.0       5039.0  ...      54322.0   \n",
       "1730        959.0       22510.0    110357.7       5565.0  ...      61213.0   \n",
       "1731         60.0       22333.0       620.0       5227.0  ...        144.0   \n",
       "1732       1630.6       22022.0    175584.1       4757.0  ...      84155.0   \n",
       "\n",
       "      청상추_가격(원/kg)  백다다기_거래량(kg)  백다다기_가격(원/kg)  애호박_거래량(kg)  애호박_가격(원/kg)  \\\n",
       "0              0.0           0.0            0.0          0.0           0.0   \n",
       "1           9235.0         434.0         2109.0      19159.0        2414.0   \n",
       "2              0.0           0.0            0.0          0.0           0.0   \n",
       "3           7631.0      500702.0         2046.0     620539.0        2018.0   \n",
       "4           6926.0      147638.0         2268.0     231958.0        2178.0   \n",
       "...            ...           ...            ...          ...           ...   \n",
       "1728        4509.0      282212.3         3001.0     313139.7        3426.0   \n",
       "1729        4178.0      312214.8         2999.0     362741.0        3357.0   \n",
       "1730        3770.0      327395.8         3065.0     390361.2        3092.0   \n",
       "1731        4076.0         285.0         3707.0       2464.0        3252.0   \n",
       "1732        4167.0      554862.6         2873.0     667745.0        2782.0   \n",
       "\n",
       "      캠벨얼리_거래량(kg)  캠벨얼리_가격(원/kg)  샤인마스캇_거래량(kg)  샤인마스캇_가격(원/kg)  \n",
       "0              0.0            0.0            0.0             0.0  \n",
       "1            880.0         2014.0            0.0             0.0  \n",
       "2              0.0            0.0            0.0             0.0  \n",
       "3           2703.8         3885.0            0.0             0.0  \n",
       "4           8810.0         2853.0            0.0             0.0  \n",
       "...            ...            ...            ...             ...  \n",
       "1728      504242.6         3620.0       283196.9         10940.0  \n",
       "1729      479683.1         3618.0       303779.6         10844.0  \n",
       "1730      521493.8         3691.0       313295.7         10636.0  \n",
       "1731       21717.0         3567.0         9734.0         10699.0  \n",
       "1732      601841.0         3761.0       382263.4         10998.0  \n",
       "\n",
       "[1733 rows x 42 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"public_data/train.csv\") \n",
    "data.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>예측대상일자</th>\n",
       "      <th>배추_가격(원/kg)</th>\n",
       "      <th>무_가격(원/kg)</th>\n",
       "      <th>양파_가격(원/kg)</th>\n",
       "      <th>건고추_가격(원/kg)</th>\n",
       "      <th>마늘_가격(원/kg)</th>\n",
       "      <th>대파_가격(원/kg)</th>\n",
       "      <th>얼갈이배추_가격(원/kg)</th>\n",
       "      <th>양배추_가격(원/kg)</th>\n",
       "      <th>깻잎_가격(원/kg)</th>\n",
       "      <th>...</th>\n",
       "      <th>당근_가격(원/kg)</th>\n",
       "      <th>파프리카_가격(원/kg)</th>\n",
       "      <th>새송이_가격(원/kg)</th>\n",
       "      <th>팽이버섯_가격(원/kg)</th>\n",
       "      <th>토마토_가격(원/kg)</th>\n",
       "      <th>청상추_가격(원/kg)</th>\n",
       "      <th>백다다기_가격(원/kg)</th>\n",
       "      <th>애호박_가격(원/kg)</th>\n",
       "      <th>캠벨얼리_가격(원/kg)</th>\n",
       "      <th>샤인마스캇_가격(원/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-29+1week</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-29+2week</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-29+4week</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-30+1week</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-30+2week</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             예측대상일자  배추_가격(원/kg)  무_가격(원/kg)  양파_가격(원/kg)  건고추_가격(원/kg)  \\\n",
       "0  2020-09-29+1week            0           0            0             0   \n",
       "1  2020-09-29+2week            0           0            0             0   \n",
       "2  2020-09-29+4week            0           0            0             0   \n",
       "3  2020-09-30+1week            0           0            0             0   \n",
       "4  2020-09-30+2week            0           0            0             0   \n",
       "\n",
       "   마늘_가격(원/kg)  대파_가격(원/kg)  얼갈이배추_가격(원/kg)  양배추_가격(원/kg)  깻잎_가격(원/kg)  ...  \\\n",
       "0            0            0               0             0            0  ...   \n",
       "1            0            0               0             0            0  ...   \n",
       "2            0            0               0             0            0  ...   \n",
       "3            0            0               0             0            0  ...   \n",
       "4            0            0               0             0            0  ...   \n",
       "\n",
       "   당근_가격(원/kg)  파프리카_가격(원/kg)  새송이_가격(원/kg)  팽이버섯_가격(원/kg)  토마토_가격(원/kg)  \\\n",
       "0            0              0             0              0             0   \n",
       "1            0              0             0              0             0   \n",
       "2            0              0             0              0             0   \n",
       "3            0              0             0              0             0   \n",
       "4            0              0             0              0             0   \n",
       "\n",
       "   청상추_가격(원/kg)  백다다기_가격(원/kg)  애호박_가격(원/kg)  캠벨얼리_가격(원/kg)  샤인마스캇_가격(원/kg)  \n",
       "0             0              0             0              0               0  \n",
       "1             0              0             0              0               0  \n",
       "2             0              0             0              0               0  \n",
       "3             0              0             0              0               0  \n",
       "4             0              0             0              0               0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('sample_submission.csv') \n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_date_list = submission[submission['예측대상일자'].str.contains('2020')]['예측대상일자'].str.split('+').str[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2020-09-29', '2020-09-30', '2020-10-01', '2020-10-02',\n",
       "       '2020-10-03', '2020-10-04', '2020-10-05', '2020-10-06',\n",
       "       '2020-10-07', '2020-10-08', '2020-10-09', '2020-10-10',\n",
       "       '2020-10-11', '2020-10-12', '2020-10-13', '2020-10-14',\n",
       "       '2020-10-15', '2020-10-16', '2020-10-17', '2020-10-18',\n",
       "       '2020-10-19', '2020-10-20', '2020-10-21', '2020-10-22',\n",
       "       '2020-10-23', '2020-10-24', '2020-10-25', '2020-10-26',\n",
       "       '2020-10-27', '2020-10-28', '2020-10-29', '2020-10-30',\n",
       "       '2020-10-31', '2020-11-01', '2020-11-02', '2020-11-03',\n",
       "       '2020-11-04', '2020-11-05'], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Date Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info = data['date'].values \n",
    "weekday_info = data['요일'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day_map = {} \n",
    "for i,d in enumerate(data['요일'].unique()):\n",
    "    week_day_map[d] = i  \n",
    "    \n",
    "    \n",
    "weekdays = [] \n",
    "for i in range(len(weekday_info)): \n",
    "    weekdays.append(week_day_map[weekday_info[i]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['2016-01-01', '2016-01-02', '2016-01-03', '2016-01-04',\n",
       "        '2016-01-05', '2016-01-06', '2016-01-07', '2016-01-08',\n",
       "        '2016-01-09', '2016-01-10'], dtype=object),\n",
       " [0, 1, 2, 3, 4, 5, 6, 0, 1, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_info[:10], weekdays[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dates = [] \n",
    "\n",
    "for i in range(len(date_info)): \n",
    "    d = date_info[i].split('-') \n",
    "    x_date = [] \n",
    "    x_date.append(int(d[0])) \n",
    "    x_date.append(int(d[1])) \n",
    "    x_date.append(int(d[2])) \n",
    "    x_date.append(int(weekdays[i])) \n",
    "    x_dates.append(x_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2016, 1, 1, 0],\n",
       " [2016, 1, 2, 1],\n",
       " [2016, 1, 3, 2],\n",
       " [2016, 1, 4, 3],\n",
       " [2016, 1, 5, 4],\n",
       " [2016, 1, 6, 5],\n",
       " [2016, 1, 7, 6],\n",
       " [2016, 1, 8, 0],\n",
       " [2016, 1, 9, 1],\n",
       " [2016, 1, 10, 2],\n",
       " [2016, 1, 11, 3],\n",
       " [2016, 1, 12, 4],\n",
       " [2016, 1, 13, 5],\n",
       " [2016, 1, 14, 6],\n",
       " [2016, 1, 15, 0],\n",
       " [2016, 1, 16, 1],\n",
       " [2016, 1, 17, 2],\n",
       " [2016, 1, 18, 3],\n",
       " [2016, 1, 19, 4],\n",
       " [2016, 1, 20, 5],\n",
       " [2016, 1, 21, 6],\n",
       " [2016, 1, 22, 0],\n",
       " [2016, 1, 23, 1],\n",
       " [2016, 1, 24, 2],\n",
       " [2016, 1, 25, 3],\n",
       " [2016, 1, 26, 4],\n",
       " [2016, 1, 27, 5],\n",
       " [2016, 1, 28, 6],\n",
       " [2016, 1, 29, 0],\n",
       " [2016, 1, 30, 1],\n",
       " [2016, 1, 31, 2],\n",
       " [2016, 2, 1, 3],\n",
       " [2016, 2, 2, 4],\n",
       " [2016, 2, 3, 5],\n",
       " [2016, 2, 4, 6],\n",
       " [2016, 2, 5, 0],\n",
       " [2016, 2, 6, 1],\n",
       " [2016, 2, 7, 2],\n",
       " [2016, 2, 8, 3],\n",
       " [2016, 2, 9, 4],\n",
       " [2016, 2, 10, 5],\n",
       " [2016, 2, 11, 6],\n",
       " [2016, 2, 12, 0],\n",
       " [2016, 2, 13, 1],\n",
       " [2016, 2, 14, 2],\n",
       " [2016, 2, 15, 3],\n",
       " [2016, 2, 16, 4],\n",
       " [2016, 2, 17, 5],\n",
       " [2016, 2, 18, 6],\n",
       " [2016, 2, 19, 0],\n",
       " [2016, 2, 20, 1],\n",
       " [2016, 2, 21, 2],\n",
       " [2016, 2, 22, 3],\n",
       " [2016, 2, 23, 4],\n",
       " [2016, 2, 24, 5],\n",
       " [2016, 2, 25, 6],\n",
       " [2016, 2, 26, 0],\n",
       " [2016, 2, 27, 1],\n",
       " [2016, 2, 28, 2],\n",
       " [2016, 2, 29, 3],\n",
       " [2016, 3, 1, 4],\n",
       " [2016, 3, 2, 5],\n",
       " [2016, 3, 3, 6],\n",
       " [2016, 3, 4, 0],\n",
       " [2016, 3, 5, 1],\n",
       " [2016, 3, 6, 2],\n",
       " [2016, 3, 7, 3],\n",
       " [2016, 3, 8, 4],\n",
       " [2016, 3, 9, 5],\n",
       " [2016, 3, 10, 6],\n",
       " [2016, 3, 11, 0],\n",
       " [2016, 3, 12, 1],\n",
       " [2016, 3, 13, 2],\n",
       " [2016, 3, 14, 3],\n",
       " [2016, 3, 15, 4],\n",
       " [2016, 3, 16, 5],\n",
       " [2016, 3, 17, 6],\n",
       " [2016, 3, 18, 0],\n",
       " [2016, 3, 19, 1],\n",
       " [2016, 3, 20, 2],\n",
       " [2016, 3, 21, 3],\n",
       " [2016, 3, 22, 4],\n",
       " [2016, 3, 23, 5],\n",
       " [2016, 3, 24, 6],\n",
       " [2016, 3, 25, 0],\n",
       " [2016, 3, 26, 1],\n",
       " [2016, 3, 27, 2],\n",
       " [2016, 3, 28, 3],\n",
       " [2016, 3, 29, 4],\n",
       " [2016, 3, 30, 5],\n",
       " [2016, 3, 31, 6],\n",
       " [2016, 4, 1, 0],\n",
       " [2016, 4, 2, 1],\n",
       " [2016, 4, 3, 2],\n",
       " [2016, 4, 4, 3],\n",
       " [2016, 4, 5, 4],\n",
       " [2016, 4, 6, 5],\n",
       " [2016, 4, 7, 6],\n",
       " [2016, 4, 8, 0],\n",
       " [2016, 4, 9, 1],\n",
       " [2016, 4, 10, 2],\n",
       " [2016, 4, 11, 3],\n",
       " [2016, 4, 12, 4],\n",
       " [2016, 4, 13, 5],\n",
       " [2016, 4, 14, 6],\n",
       " [2016, 4, 15, 0],\n",
       " [2016, 4, 16, 1],\n",
       " [2016, 4, 17, 2],\n",
       " [2016, 4, 18, 3],\n",
       " [2016, 4, 19, 4],\n",
       " [2016, 4, 20, 5],\n",
       " [2016, 4, 21, 6],\n",
       " [2016, 4, 22, 0],\n",
       " [2016, 4, 23, 1],\n",
       " [2016, 4, 24, 2],\n",
       " [2016, 4, 25, 3],\n",
       " [2016, 4, 26, 4],\n",
       " [2016, 4, 27, 5],\n",
       " [2016, 4, 28, 6],\n",
       " [2016, 4, 29, 0],\n",
       " [2016, 4, 30, 1],\n",
       " [2016, 5, 1, 2],\n",
       " [2016, 5, 2, 3],\n",
       " [2016, 5, 3, 4],\n",
       " [2016, 5, 4, 5],\n",
       " [2016, 5, 5, 6],\n",
       " [2016, 5, 6, 0],\n",
       " [2016, 5, 7, 1],\n",
       " [2016, 5, 8, 2],\n",
       " [2016, 5, 9, 3],\n",
       " [2016, 5, 10, 4],\n",
       " [2016, 5, 11, 5],\n",
       " [2016, 5, 12, 6],\n",
       " [2016, 5, 13, 0],\n",
       " [2016, 5, 14, 1],\n",
       " [2016, 5, 15, 2],\n",
       " [2016, 5, 16, 3],\n",
       " [2016, 5, 17, 4],\n",
       " [2016, 5, 18, 5],\n",
       " [2016, 5, 19, 6],\n",
       " [2016, 5, 20, 0],\n",
       " [2016, 5, 21, 1],\n",
       " [2016, 5, 22, 2],\n",
       " [2016, 5, 23, 3],\n",
       " [2016, 5, 24, 4],\n",
       " [2016, 5, 25, 5],\n",
       " [2016, 5, 26, 6],\n",
       " [2016, 5, 27, 0],\n",
       " [2016, 5, 28, 1],\n",
       " [2016, 5, 29, 2],\n",
       " [2016, 5, 30, 3],\n",
       " [2016, 5, 31, 4],\n",
       " [2016, 6, 1, 5],\n",
       " [2016, 6, 2, 6],\n",
       " [2016, 6, 3, 0],\n",
       " [2016, 6, 4, 1],\n",
       " [2016, 6, 5, 2],\n",
       " [2016, 6, 6, 3],\n",
       " [2016, 6, 7, 4],\n",
       " [2016, 6, 8, 5],\n",
       " [2016, 6, 9, 6],\n",
       " [2016, 6, 10, 0],\n",
       " [2016, 6, 11, 1],\n",
       " [2016, 6, 12, 2],\n",
       " [2016, 6, 13, 3],\n",
       " [2016, 6, 14, 4],\n",
       " [2016, 6, 15, 5],\n",
       " [2016, 6, 16, 6],\n",
       " [2016, 6, 17, 0],\n",
       " [2016, 6, 18, 1],\n",
       " [2016, 6, 19, 2],\n",
       " [2016, 6, 20, 3],\n",
       " [2016, 6, 21, 4],\n",
       " [2016, 6, 22, 5],\n",
       " [2016, 6, 23, 6],\n",
       " [2016, 6, 24, 0],\n",
       " [2016, 6, 25, 1],\n",
       " [2016, 6, 26, 2],\n",
       " [2016, 6, 27, 3],\n",
       " [2016, 6, 28, 4],\n",
       " [2016, 6, 29, 5],\n",
       " [2016, 6, 30, 6],\n",
       " [2016, 7, 1, 0],\n",
       " [2016, 7, 2, 1],\n",
       " [2016, 7, 3, 2],\n",
       " [2016, 7, 4, 3],\n",
       " [2016, 7, 5, 4],\n",
       " [2016, 7, 6, 5],\n",
       " [2016, 7, 7, 6],\n",
       " [2016, 7, 8, 0],\n",
       " [2016, 7, 9, 1],\n",
       " [2016, 7, 10, 2],\n",
       " [2016, 7, 11, 3],\n",
       " [2016, 7, 12, 4],\n",
       " [2016, 7, 13, 5],\n",
       " [2016, 7, 14, 6],\n",
       " [2016, 7, 15, 0],\n",
       " [2016, 7, 16, 1],\n",
       " [2016, 7, 17, 2],\n",
       " [2016, 7, 18, 3],\n",
       " [2016, 7, 19, 4],\n",
       " [2016, 7, 20, 5],\n",
       " [2016, 7, 21, 6],\n",
       " [2016, 7, 22, 0],\n",
       " [2016, 7, 23, 1],\n",
       " [2016, 7, 24, 2],\n",
       " [2016, 7, 25, 3],\n",
       " [2016, 7, 26, 4],\n",
       " [2016, 7, 27, 5],\n",
       " [2016, 7, 28, 6],\n",
       " [2016, 7, 29, 0],\n",
       " [2016, 7, 30, 1],\n",
       " [2016, 7, 31, 2],\n",
       " [2016, 8, 1, 3],\n",
       " [2016, 8, 2, 4],\n",
       " [2016, 8, 3, 5],\n",
       " [2016, 8, 4, 6],\n",
       " [2016, 8, 5, 0],\n",
       " [2016, 8, 6, 1],\n",
       " [2016, 8, 7, 2],\n",
       " [2016, 8, 8, 3],\n",
       " [2016, 8, 9, 4],\n",
       " [2016, 8, 10, 5],\n",
       " [2016, 8, 11, 6],\n",
       " [2016, 8, 12, 0],\n",
       " [2016, 8, 13, 1],\n",
       " [2016, 8, 14, 2],\n",
       " [2016, 8, 15, 3],\n",
       " [2016, 8, 16, 4],\n",
       " [2016, 8, 17, 5],\n",
       " [2016, 8, 18, 6],\n",
       " [2016, 8, 19, 0],\n",
       " [2016, 8, 20, 1],\n",
       " [2016, 8, 21, 2],\n",
       " [2016, 8, 22, 3],\n",
       " [2016, 8, 23, 4],\n",
       " [2016, 8, 24, 5],\n",
       " [2016, 8, 25, 6],\n",
       " [2016, 8, 26, 0],\n",
       " [2016, 8, 27, 1],\n",
       " [2016, 8, 28, 2],\n",
       " [2016, 8, 29, 3],\n",
       " [2016, 8, 30, 4],\n",
       " [2016, 8, 31, 5],\n",
       " [2016, 9, 1, 6],\n",
       " [2016, 9, 2, 0],\n",
       " [2016, 9, 3, 1],\n",
       " [2016, 9, 4, 2],\n",
       " [2016, 9, 5, 3],\n",
       " [2016, 9, 6, 4],\n",
       " [2016, 9, 7, 5],\n",
       " [2016, 9, 8, 6],\n",
       " [2016, 9, 9, 0],\n",
       " [2016, 9, 10, 1],\n",
       " [2016, 9, 11, 2],\n",
       " [2016, 9, 12, 3],\n",
       " [2016, 9, 13, 4],\n",
       " [2016, 9, 14, 5],\n",
       " [2016, 9, 15, 6],\n",
       " [2016, 9, 16, 0],\n",
       " [2016, 9, 17, 1],\n",
       " [2016, 9, 18, 2],\n",
       " [2016, 9, 19, 3],\n",
       " [2016, 9, 20, 4],\n",
       " [2016, 9, 21, 5],\n",
       " [2016, 9, 22, 6],\n",
       " [2016, 9, 23, 0],\n",
       " [2016, 9, 24, 1],\n",
       " [2016, 9, 25, 2],\n",
       " [2016, 9, 26, 3],\n",
       " [2016, 9, 27, 4],\n",
       " [2016, 9, 28, 5],\n",
       " [2016, 9, 29, 6],\n",
       " [2016, 9, 30, 0],\n",
       " [2016, 10, 1, 1],\n",
       " [2016, 10, 2, 2],\n",
       " [2016, 10, 3, 3],\n",
       " [2016, 10, 4, 4],\n",
       " [2016, 10, 5, 5],\n",
       " [2016, 10, 6, 6],\n",
       " [2016, 10, 7, 0],\n",
       " [2016, 10, 8, 1],\n",
       " [2016, 10, 9, 2],\n",
       " [2016, 10, 10, 3],\n",
       " [2016, 10, 11, 4],\n",
       " [2016, 10, 12, 5],\n",
       " [2016, 10, 13, 6],\n",
       " [2016, 10, 14, 0],\n",
       " [2016, 10, 15, 1],\n",
       " [2016, 10, 16, 2],\n",
       " [2016, 10, 17, 3],\n",
       " [2016, 10, 18, 4],\n",
       " [2016, 10, 19, 5],\n",
       " [2016, 10, 20, 6],\n",
       " [2016, 10, 21, 0],\n",
       " [2016, 10, 22, 1],\n",
       " [2016, 10, 23, 2],\n",
       " [2016, 10, 24, 3],\n",
       " [2016, 10, 25, 4],\n",
       " [2016, 10, 26, 5],\n",
       " [2016, 10, 27, 6],\n",
       " [2016, 10, 28, 0],\n",
       " [2016, 10, 29, 1],\n",
       " [2016, 10, 30, 2],\n",
       " [2016, 10, 31, 3],\n",
       " [2016, 11, 1, 4],\n",
       " [2016, 11, 2, 5],\n",
       " [2016, 11, 3, 6],\n",
       " [2016, 11, 4, 0],\n",
       " [2016, 11, 5, 1],\n",
       " [2016, 11, 6, 2],\n",
       " [2016, 11, 7, 3],\n",
       " [2016, 11, 8, 4],\n",
       " [2016, 11, 9, 5],\n",
       " [2016, 11, 10, 6],\n",
       " [2016, 11, 11, 0],\n",
       " [2016, 11, 12, 1],\n",
       " [2016, 11, 13, 2],\n",
       " [2016, 11, 14, 3],\n",
       " [2016, 11, 15, 4],\n",
       " [2016, 11, 16, 5],\n",
       " [2016, 11, 17, 6],\n",
       " [2016, 11, 18, 0],\n",
       " [2016, 11, 19, 1],\n",
       " [2016, 11, 20, 2],\n",
       " [2016, 11, 21, 3],\n",
       " [2016, 11, 22, 4],\n",
       " [2016, 11, 23, 5],\n",
       " [2016, 11, 24, 6],\n",
       " [2016, 11, 25, 0],\n",
       " [2016, 11, 26, 1],\n",
       " [2016, 11, 27, 2],\n",
       " [2016, 11, 28, 3],\n",
       " [2016, 11, 29, 4],\n",
       " [2016, 11, 30, 5],\n",
       " [2016, 12, 1, 6],\n",
       " [2016, 12, 2, 0],\n",
       " [2016, 12, 3, 1],\n",
       " [2016, 12, 4, 2],\n",
       " [2016, 12, 5, 3],\n",
       " [2016, 12, 6, 4],\n",
       " [2016, 12, 7, 5],\n",
       " [2016, 12, 8, 6],\n",
       " [2016, 12, 9, 0],\n",
       " [2016, 12, 10, 1],\n",
       " [2016, 12, 11, 2],\n",
       " [2016, 12, 12, 3],\n",
       " [2016, 12, 13, 4],\n",
       " [2016, 12, 14, 5],\n",
       " [2016, 12, 15, 6],\n",
       " [2016, 12, 16, 0],\n",
       " [2016, 12, 17, 1],\n",
       " [2016, 12, 18, 2],\n",
       " [2016, 12, 19, 3],\n",
       " [2016, 12, 20, 4],\n",
       " [2016, 12, 21, 5],\n",
       " [2016, 12, 22, 6],\n",
       " [2016, 12, 23, 0],\n",
       " [2016, 12, 24, 1],\n",
       " [2016, 12, 25, 2],\n",
       " [2016, 12, 26, 3],\n",
       " [2016, 12, 27, 4],\n",
       " [2016, 12, 28, 5],\n",
       " [2016, 12, 29, 6],\n",
       " [2016, 12, 30, 0],\n",
       " [2016, 12, 31, 1],\n",
       " [2017, 1, 1, 2],\n",
       " [2017, 1, 2, 3],\n",
       " [2017, 1, 3, 4],\n",
       " [2017, 1, 4, 5],\n",
       " [2017, 1, 5, 6],\n",
       " [2017, 1, 6, 0],\n",
       " [2017, 1, 7, 1],\n",
       " [2017, 1, 8, 2],\n",
       " [2017, 1, 9, 3],\n",
       " [2017, 1, 10, 4],\n",
       " [2017, 1, 11, 5],\n",
       " [2017, 1, 12, 6],\n",
       " [2017, 1, 13, 0],\n",
       " [2017, 1, 14, 1],\n",
       " [2017, 1, 15, 2],\n",
       " [2017, 1, 16, 3],\n",
       " [2017, 1, 17, 4],\n",
       " [2017, 1, 18, 5],\n",
       " [2017, 1, 19, 6],\n",
       " [2017, 1, 20, 0],\n",
       " [2017, 1, 21, 1],\n",
       " [2017, 1, 22, 2],\n",
       " [2017, 1, 23, 3],\n",
       " [2017, 1, 24, 4],\n",
       " [2017, 1, 25, 5],\n",
       " [2017, 1, 26, 6],\n",
       " [2017, 1, 27, 0],\n",
       " [2017, 1, 28, 1],\n",
       " [2017, 1, 29, 2],\n",
       " [2017, 1, 30, 3],\n",
       " [2017, 1, 31, 4],\n",
       " [2017, 2, 1, 5],\n",
       " [2017, 2, 2, 6],\n",
       " [2017, 2, 3, 0],\n",
       " [2017, 2, 4, 1],\n",
       " [2017, 2, 5, 2],\n",
       " [2017, 2, 6, 3],\n",
       " [2017, 2, 7, 4],\n",
       " [2017, 2, 8, 5],\n",
       " [2017, 2, 9, 6],\n",
       " [2017, 2, 10, 0],\n",
       " [2017, 2, 11, 1],\n",
       " [2017, 2, 12, 2],\n",
       " [2017, 2, 13, 3],\n",
       " [2017, 2, 14, 4],\n",
       " [2017, 2, 15, 5],\n",
       " [2017, 2, 16, 6],\n",
       " [2017, 2, 17, 0],\n",
       " [2017, 2, 18, 1],\n",
       " [2017, 2, 19, 2],\n",
       " [2017, 2, 20, 3],\n",
       " [2017, 2, 21, 4],\n",
       " [2017, 2, 22, 5],\n",
       " [2017, 2, 23, 6],\n",
       " [2017, 2, 24, 0],\n",
       " [2017, 2, 25, 1],\n",
       " [2017, 2, 26, 2],\n",
       " [2017, 2, 27, 3],\n",
       " [2017, 2, 28, 4],\n",
       " [2017, 3, 1, 5],\n",
       " [2017, 3, 2, 6],\n",
       " [2017, 3, 3, 0],\n",
       " [2017, 3, 4, 1],\n",
       " [2017, 3, 5, 2],\n",
       " [2017, 3, 6, 3],\n",
       " [2017, 3, 7, 4],\n",
       " [2017, 3, 8, 5],\n",
       " [2017, 3, 9, 6],\n",
       " [2017, 3, 10, 0],\n",
       " [2017, 3, 11, 1],\n",
       " [2017, 3, 12, 2],\n",
       " [2017, 3, 13, 3],\n",
       " [2017, 3, 14, 4],\n",
       " [2017, 3, 15, 5],\n",
       " [2017, 3, 16, 6],\n",
       " [2017, 3, 17, 0],\n",
       " [2017, 3, 18, 1],\n",
       " [2017, 3, 19, 2],\n",
       " [2017, 3, 20, 3],\n",
       " [2017, 3, 21, 4],\n",
       " [2017, 3, 22, 5],\n",
       " [2017, 3, 23, 6],\n",
       " [2017, 3, 24, 0],\n",
       " [2017, 3, 25, 1],\n",
       " [2017, 3, 26, 2],\n",
       " [2017, 3, 27, 3],\n",
       " [2017, 3, 28, 4],\n",
       " [2017, 3, 29, 5],\n",
       " [2017, 3, 30, 6],\n",
       " [2017, 3, 31, 0],\n",
       " [2017, 4, 1, 1],\n",
       " [2017, 4, 2, 2],\n",
       " [2017, 4, 3, 3],\n",
       " [2017, 4, 4, 4],\n",
       " [2017, 4, 5, 5],\n",
       " [2017, 4, 6, 6],\n",
       " [2017, 4, 7, 0],\n",
       " [2017, 4, 8, 1],\n",
       " [2017, 4, 9, 2],\n",
       " [2017, 4, 10, 3],\n",
       " [2017, 4, 11, 4],\n",
       " [2017, 4, 12, 5],\n",
       " [2017, 4, 13, 6],\n",
       " [2017, 4, 14, 0],\n",
       " [2017, 4, 15, 1],\n",
       " [2017, 4, 16, 2],\n",
       " [2017, 4, 17, 3],\n",
       " [2017, 4, 18, 4],\n",
       " [2017, 4, 19, 5],\n",
       " [2017, 4, 20, 6],\n",
       " [2017, 4, 21, 0],\n",
       " [2017, 4, 22, 1],\n",
       " [2017, 4, 23, 2],\n",
       " [2017, 4, 24, 3],\n",
       " [2017, 4, 25, 4],\n",
       " [2017, 4, 26, 5],\n",
       " [2017, 4, 27, 6],\n",
       " [2017, 4, 28, 0],\n",
       " [2017, 4, 29, 1],\n",
       " [2017, 4, 30, 2],\n",
       " [2017, 5, 1, 3],\n",
       " [2017, 5, 2, 4],\n",
       " [2017, 5, 3, 5],\n",
       " [2017, 5, 4, 6],\n",
       " [2017, 5, 5, 0],\n",
       " [2017, 5, 6, 1],\n",
       " [2017, 5, 7, 2],\n",
       " [2017, 5, 8, 3],\n",
       " [2017, 5, 9, 4],\n",
       " [2017, 5, 10, 5],\n",
       " [2017, 5, 11, 6],\n",
       " [2017, 5, 12, 0],\n",
       " [2017, 5, 13, 1],\n",
       " [2017, 5, 14, 2],\n",
       " [2017, 5, 15, 3],\n",
       " [2017, 5, 16, 4],\n",
       " [2017, 5, 17, 5],\n",
       " [2017, 5, 18, 6],\n",
       " [2017, 5, 19, 0],\n",
       " [2017, 5, 20, 1],\n",
       " [2017, 5, 21, 2],\n",
       " [2017, 5, 22, 3],\n",
       " [2017, 5, 23, 4],\n",
       " [2017, 5, 24, 5],\n",
       " [2017, 5, 25, 6],\n",
       " [2017, 5, 26, 0],\n",
       " [2017, 5, 27, 1],\n",
       " [2017, 5, 28, 2],\n",
       " [2017, 5, 29, 3],\n",
       " [2017, 5, 30, 4],\n",
       " [2017, 5, 31, 5],\n",
       " [2017, 6, 1, 6],\n",
       " [2017, 6, 2, 0],\n",
       " [2017, 6, 3, 1],\n",
       " [2017, 6, 4, 2],\n",
       " [2017, 6, 5, 3],\n",
       " [2017, 6, 6, 4],\n",
       " [2017, 6, 7, 5],\n",
       " [2017, 6, 8, 6],\n",
       " [2017, 6, 9, 0],\n",
       " [2017, 6, 10, 1],\n",
       " [2017, 6, 11, 2],\n",
       " [2017, 6, 12, 3],\n",
       " [2017, 6, 13, 4],\n",
       " [2017, 6, 14, 5],\n",
       " [2017, 6, 15, 6],\n",
       " [2017, 6, 16, 0],\n",
       " [2017, 6, 17, 1],\n",
       " [2017, 6, 18, 2],\n",
       " [2017, 6, 19, 3],\n",
       " [2017, 6, 20, 4],\n",
       " [2017, 6, 21, 5],\n",
       " [2017, 6, 22, 6],\n",
       " [2017, 6, 23, 0],\n",
       " [2017, 6, 24, 1],\n",
       " [2017, 6, 25, 2],\n",
       " [2017, 6, 26, 3],\n",
       " [2017, 6, 27, 4],\n",
       " [2017, 6, 28, 5],\n",
       " [2017, 6, 29, 6],\n",
       " [2017, 6, 30, 0],\n",
       " [2017, 7, 1, 1],\n",
       " [2017, 7, 2, 2],\n",
       " [2017, 7, 3, 3],\n",
       " [2017, 7, 4, 4],\n",
       " [2017, 7, 5, 5],\n",
       " [2017, 7, 6, 6],\n",
       " [2017, 7, 7, 0],\n",
       " [2017, 7, 8, 1],\n",
       " [2017, 7, 9, 2],\n",
       " [2017, 7, 10, 3],\n",
       " [2017, 7, 11, 4],\n",
       " [2017, 7, 12, 5],\n",
       " [2017, 7, 13, 6],\n",
       " [2017, 7, 14, 0],\n",
       " [2017, 7, 15, 1],\n",
       " [2017, 7, 16, 2],\n",
       " [2017, 7, 17, 3],\n",
       " [2017, 7, 18, 4],\n",
       " [2017, 7, 19, 5],\n",
       " [2017, 7, 20, 6],\n",
       " [2017, 7, 21, 0],\n",
       " [2017, 7, 22, 1],\n",
       " [2017, 7, 23, 2],\n",
       " [2017, 7, 24, 3],\n",
       " [2017, 7, 25, 4],\n",
       " [2017, 7, 26, 5],\n",
       " [2017, 7, 27, 6],\n",
       " [2017, 7, 28, 0],\n",
       " [2017, 7, 29, 1],\n",
       " [2017, 7, 30, 2],\n",
       " [2017, 7, 31, 3],\n",
       " [2017, 8, 1, 4],\n",
       " [2017, 8, 2, 5],\n",
       " [2017, 8, 3, 6],\n",
       " [2017, 8, 4, 0],\n",
       " [2017, 8, 5, 1],\n",
       " [2017, 8, 6, 2],\n",
       " [2017, 8, 7, 3],\n",
       " [2017, 8, 8, 4],\n",
       " [2017, 8, 9, 5],\n",
       " [2017, 8, 10, 6],\n",
       " [2017, 8, 11, 0],\n",
       " [2017, 8, 12, 1],\n",
       " [2017, 8, 13, 2],\n",
       " [2017, 8, 14, 3],\n",
       " [2017, 8, 15, 4],\n",
       " [2017, 8, 16, 5],\n",
       " [2017, 8, 17, 6],\n",
       " [2017, 8, 18, 0],\n",
       " [2017, 8, 19, 1],\n",
       " [2017, 8, 20, 2],\n",
       " [2017, 8, 21, 3],\n",
       " [2017, 8, 22, 4],\n",
       " [2017, 8, 23, 5],\n",
       " [2017, 8, 24, 6],\n",
       " [2017, 8, 25, 0],\n",
       " [2017, 8, 26, 1],\n",
       " [2017, 8, 27, 2],\n",
       " [2017, 8, 28, 3],\n",
       " [2017, 8, 29, 4],\n",
       " [2017, 8, 30, 5],\n",
       " [2017, 8, 31, 6],\n",
       " [2017, 9, 1, 0],\n",
       " [2017, 9, 2, 1],\n",
       " [2017, 9, 3, 2],\n",
       " [2017, 9, 4, 3],\n",
       " [2017, 9, 5, 4],\n",
       " [2017, 9, 6, 5],\n",
       " [2017, 9, 7, 6],\n",
       " [2017, 9, 8, 0],\n",
       " [2017, 9, 9, 1],\n",
       " [2017, 9, 10, 2],\n",
       " [2017, 9, 11, 3],\n",
       " [2017, 9, 12, 4],\n",
       " [2017, 9, 13, 5],\n",
       " [2017, 9, 14, 6],\n",
       " [2017, 9, 15, 0],\n",
       " [2017, 9, 16, 1],\n",
       " [2017, 9, 17, 2],\n",
       " [2017, 9, 18, 3],\n",
       " [2017, 9, 19, 4],\n",
       " [2017, 9, 20, 5],\n",
       " [2017, 9, 21, 6],\n",
       " [2017, 9, 22, 0],\n",
       " [2017, 9, 23, 1],\n",
       " [2017, 9, 24, 2],\n",
       " [2017, 9, 25, 3],\n",
       " [2017, 9, 26, 4],\n",
       " [2017, 9, 27, 5],\n",
       " [2017, 9, 28, 6],\n",
       " [2017, 9, 29, 0],\n",
       " [2017, 9, 30, 1],\n",
       " [2017, 10, 1, 2],\n",
       " [2017, 10, 2, 3],\n",
       " [2017, 10, 3, 4],\n",
       " [2017, 10, 4, 5],\n",
       " [2017, 10, 5, 6],\n",
       " [2017, 10, 6, 0],\n",
       " [2017, 10, 7, 1],\n",
       " [2017, 10, 8, 2],\n",
       " [2017, 10, 9, 3],\n",
       " [2017, 10, 10, 4],\n",
       " [2017, 10, 11, 5],\n",
       " [2017, 10, 12, 6],\n",
       " [2017, 10, 13, 0],\n",
       " [2017, 10, 14, 1],\n",
       " [2017, 10, 15, 2],\n",
       " [2017, 10, 16, 3],\n",
       " [2017, 10, 17, 4],\n",
       " [2017, 10, 18, 5],\n",
       " [2017, 10, 19, 6],\n",
       " [2017, 10, 20, 0],\n",
       " [2017, 10, 21, 1],\n",
       " [2017, 10, 22, 2],\n",
       " [2017, 10, 23, 3],\n",
       " [2017, 10, 24, 4],\n",
       " [2017, 10, 25, 5],\n",
       " [2017, 10, 26, 6],\n",
       " [2017, 10, 27, 0],\n",
       " [2017, 10, 28, 1],\n",
       " [2017, 10, 29, 2],\n",
       " [2017, 10, 30, 3],\n",
       " [2017, 10, 31, 4],\n",
       " [2017, 11, 1, 5],\n",
       " [2017, 11, 2, 6],\n",
       " [2017, 11, 3, 0],\n",
       " [2017, 11, 4, 1],\n",
       " [2017, 11, 5, 2],\n",
       " [2017, 11, 6, 3],\n",
       " [2017, 11, 7, 4],\n",
       " [2017, 11, 8, 5],\n",
       " [2017, 11, 9, 6],\n",
       " [2017, 11, 10, 0],\n",
       " [2017, 11, 11, 1],\n",
       " [2017, 11, 12, 2],\n",
       " [2017, 11, 13, 3],\n",
       " [2017, 11, 14, 4],\n",
       " [2017, 11, 15, 5],\n",
       " [2017, 11, 16, 6],\n",
       " [2017, 11, 17, 0],\n",
       " [2017, 11, 18, 1],\n",
       " [2017, 11, 19, 2],\n",
       " [2017, 11, 20, 3],\n",
       " [2017, 11, 21, 4],\n",
       " [2017, 11, 22, 5],\n",
       " [2017, 11, 23, 6],\n",
       " [2017, 11, 24, 0],\n",
       " [2017, 11, 25, 1],\n",
       " [2017, 11, 26, 2],\n",
       " [2017, 11, 27, 3],\n",
       " [2017, 11, 28, 4],\n",
       " [2017, 11, 29, 5],\n",
       " [2017, 11, 30, 6],\n",
       " [2017, 12, 1, 0],\n",
       " [2017, 12, 2, 1],\n",
       " [2017, 12, 3, 2],\n",
       " [2017, 12, 4, 3],\n",
       " [2017, 12, 5, 4],\n",
       " [2017, 12, 6, 5],\n",
       " [2017, 12, 7, 6],\n",
       " [2017, 12, 8, 0],\n",
       " [2017, 12, 9, 1],\n",
       " [2017, 12, 10, 2],\n",
       " [2017, 12, 11, 3],\n",
       " [2017, 12, 12, 4],\n",
       " [2017, 12, 13, 5],\n",
       " [2017, 12, 14, 6],\n",
       " [2017, 12, 15, 0],\n",
       " [2017, 12, 16, 1],\n",
       " [2017, 12, 17, 2],\n",
       " [2017, 12, 18, 3],\n",
       " [2017, 12, 19, 4],\n",
       " [2017, 12, 20, 5],\n",
       " [2017, 12, 21, 6],\n",
       " [2017, 12, 22, 0],\n",
       " [2017, 12, 23, 1],\n",
       " [2017, 12, 24, 2],\n",
       " [2017, 12, 25, 3],\n",
       " [2017, 12, 26, 4],\n",
       " [2017, 12, 27, 5],\n",
       " [2017, 12, 28, 6],\n",
       " [2017, 12, 29, 0],\n",
       " [2017, 12, 30, 1],\n",
       " [2017, 12, 31, 2],\n",
       " [2018, 1, 1, 3],\n",
       " [2018, 1, 2, 4],\n",
       " [2018, 1, 3, 5],\n",
       " [2018, 1, 4, 6],\n",
       " [2018, 1, 5, 0],\n",
       " [2018, 1, 6, 1],\n",
       " [2018, 1, 7, 2],\n",
       " [2018, 1, 8, 3],\n",
       " [2018, 1, 9, 4],\n",
       " [2018, 1, 10, 5],\n",
       " [2018, 1, 11, 6],\n",
       " [2018, 1, 12, 0],\n",
       " [2018, 1, 13, 1],\n",
       " [2018, 1, 14, 2],\n",
       " [2018, 1, 15, 3],\n",
       " [2018, 1, 16, 4],\n",
       " [2018, 1, 17, 5],\n",
       " [2018, 1, 18, 6],\n",
       " [2018, 1, 19, 0],\n",
       " [2018, 1, 20, 1],\n",
       " [2018, 1, 21, 2],\n",
       " [2018, 1, 22, 3],\n",
       " [2018, 1, 23, 4],\n",
       " [2018, 1, 24, 5],\n",
       " [2018, 1, 25, 6],\n",
       " [2018, 1, 26, 0],\n",
       " [2018, 1, 27, 1],\n",
       " [2018, 1, 28, 2],\n",
       " [2018, 1, 29, 3],\n",
       " [2018, 1, 30, 4],\n",
       " [2018, 1, 31, 5],\n",
       " [2018, 2, 1, 6],\n",
       " [2018, 2, 2, 0],\n",
       " [2018, 2, 3, 1],\n",
       " [2018, 2, 4, 2],\n",
       " [2018, 2, 5, 3],\n",
       " [2018, 2, 6, 4],\n",
       " [2018, 2, 7, 5],\n",
       " [2018, 2, 8, 6],\n",
       " [2018, 2, 9, 0],\n",
       " [2018, 2, 10, 1],\n",
       " [2018, 2, 11, 2],\n",
       " [2018, 2, 12, 3],\n",
       " [2018, 2, 13, 4],\n",
       " [2018, 2, 14, 5],\n",
       " [2018, 2, 15, 6],\n",
       " [2018, 2, 16, 0],\n",
       " [2018, 2, 17, 1],\n",
       " [2018, 2, 18, 2],\n",
       " [2018, 2, 19, 3],\n",
       " [2018, 2, 20, 4],\n",
       " [2018, 2, 21, 5],\n",
       " [2018, 2, 22, 6],\n",
       " [2018, 2, 23, 0],\n",
       " [2018, 2, 24, 1],\n",
       " [2018, 2, 25, 2],\n",
       " [2018, 2, 26, 3],\n",
       " [2018, 2, 27, 4],\n",
       " [2018, 2, 28, 5],\n",
       " [2018, 3, 1, 6],\n",
       " [2018, 3, 2, 0],\n",
       " [2018, 3, 3, 1],\n",
       " [2018, 3, 4, 2],\n",
       " [2018, 3, 5, 3],\n",
       " [2018, 3, 6, 4],\n",
       " [2018, 3, 7, 5],\n",
       " [2018, 3, 8, 6],\n",
       " [2018, 3, 9, 0],\n",
       " [2018, 3, 10, 1],\n",
       " [2018, 3, 11, 2],\n",
       " [2018, 3, 12, 3],\n",
       " [2018, 3, 13, 4],\n",
       " [2018, 3, 14, 5],\n",
       " [2018, 3, 15, 6],\n",
       " [2018, 3, 16, 0],\n",
       " [2018, 3, 17, 1],\n",
       " [2018, 3, 18, 2],\n",
       " [2018, 3, 19, 3],\n",
       " [2018, 3, 20, 4],\n",
       " [2018, 3, 21, 5],\n",
       " [2018, 3, 22, 6],\n",
       " [2018, 3, 23, 0],\n",
       " [2018, 3, 24, 1],\n",
       " [2018, 3, 25, 2],\n",
       " [2018, 3, 26, 3],\n",
       " [2018, 3, 27, 4],\n",
       " [2018, 3, 28, 5],\n",
       " [2018, 3, 29, 6],\n",
       " [2018, 3, 30, 0],\n",
       " [2018, 3, 31, 1],\n",
       " [2018, 4, 1, 2],\n",
       " [2018, 4, 2, 3],\n",
       " [2018, 4, 3, 4],\n",
       " [2018, 4, 4, 5],\n",
       " [2018, 4, 5, 6],\n",
       " [2018, 4, 6, 0],\n",
       " [2018, 4, 7, 1],\n",
       " [2018, 4, 8, 2],\n",
       " [2018, 4, 9, 3],\n",
       " [2018, 4, 10, 4],\n",
       " [2018, 4, 11, 5],\n",
       " [2018, 4, 12, 6],\n",
       " [2018, 4, 13, 0],\n",
       " [2018, 4, 14, 1],\n",
       " [2018, 4, 15, 2],\n",
       " [2018, 4, 16, 3],\n",
       " [2018, 4, 17, 4],\n",
       " [2018, 4, 18, 5],\n",
       " [2018, 4, 19, 6],\n",
       " [2018, 4, 20, 0],\n",
       " [2018, 4, 21, 1],\n",
       " [2018, 4, 22, 2],\n",
       " [2018, 4, 23, 3],\n",
       " [2018, 4, 24, 4],\n",
       " [2018, 4, 25, 5],\n",
       " [2018, 4, 26, 6],\n",
       " [2018, 4, 27, 0],\n",
       " [2018, 4, 28, 1],\n",
       " [2018, 4, 29, 2],\n",
       " [2018, 4, 30, 3],\n",
       " [2018, 5, 1, 4],\n",
       " [2018, 5, 2, 5],\n",
       " [2018, 5, 3, 6],\n",
       " [2018, 5, 4, 0],\n",
       " [2018, 5, 5, 1],\n",
       " [2018, 5, 6, 2],\n",
       " [2018, 5, 7, 3],\n",
       " [2018, 5, 8, 4],\n",
       " [2018, 5, 9, 5],\n",
       " [2018, 5, 10, 6],\n",
       " [2018, 5, 11, 0],\n",
       " [2018, 5, 12, 1],\n",
       " [2018, 5, 13, 2],\n",
       " [2018, 5, 14, 3],\n",
       " [2018, 5, 15, 4],\n",
       " [2018, 5, 16, 5],\n",
       " [2018, 5, 17, 6],\n",
       " [2018, 5, 18, 0],\n",
       " [2018, 5, 19, 1],\n",
       " [2018, 5, 20, 2],\n",
       " [2018, 5, 21, 3],\n",
       " [2018, 5, 22, 4],\n",
       " [2018, 5, 23, 5],\n",
       " [2018, 5, 24, 6],\n",
       " [2018, 5, 25, 0],\n",
       " [2018, 5, 26, 1],\n",
       " [2018, 5, 27, 2],\n",
       " [2018, 5, 28, 3],\n",
       " [2018, 5, 29, 4],\n",
       " [2018, 5, 30, 5],\n",
       " [2018, 5, 31, 6],\n",
       " [2018, 6, 1, 0],\n",
       " [2018, 6, 2, 1],\n",
       " [2018, 6, 3, 2],\n",
       " [2018, 6, 4, 3],\n",
       " [2018, 6, 5, 4],\n",
       " [2018, 6, 6, 5],\n",
       " [2018, 6, 7, 6],\n",
       " [2018, 6, 8, 0],\n",
       " [2018, 6, 9, 1],\n",
       " [2018, 6, 10, 2],\n",
       " [2018, 6, 11, 3],\n",
       " [2018, 6, 12, 4],\n",
       " [2018, 6, 13, 5],\n",
       " [2018, 6, 14, 6],\n",
       " [2018, 6, 15, 0],\n",
       " [2018, 6, 16, 1],\n",
       " [2018, 6, 17, 2],\n",
       " [2018, 6, 18, 3],\n",
       " [2018, 6, 19, 4],\n",
       " [2018, 6, 20, 5],\n",
       " [2018, 6, 21, 6],\n",
       " [2018, 6, 22, 0],\n",
       " [2018, 6, 23, 1],\n",
       " [2018, 6, 24, 2],\n",
       " [2018, 6, 25, 3],\n",
       " [2018, 6, 26, 4],\n",
       " [2018, 6, 27, 5],\n",
       " [2018, 6, 28, 6],\n",
       " [2018, 6, 29, 0],\n",
       " [2018, 6, 30, 1],\n",
       " [2018, 7, 1, 2],\n",
       " [2018, 7, 2, 3],\n",
       " [2018, 7, 3, 4],\n",
       " [2018, 7, 4, 5],\n",
       " [2018, 7, 5, 6],\n",
       " [2018, 7, 6, 0],\n",
       " [2018, 7, 7, 1],\n",
       " [2018, 7, 8, 2],\n",
       " [2018, 7, 9, 3],\n",
       " [2018, 7, 10, 4],\n",
       " [2018, 7, 11, 5],\n",
       " [2018, 7, 12, 6],\n",
       " [2018, 7, 13, 0],\n",
       " [2018, 7, 14, 1],\n",
       " [2018, 7, 15, 2],\n",
       " [2018, 7, 16, 3],\n",
       " [2018, 7, 17, 4],\n",
       " [2018, 7, 18, 5],\n",
       " [2018, 7, 19, 6],\n",
       " [2018, 7, 20, 0],\n",
       " [2018, 7, 21, 1],\n",
       " [2018, 7, 22, 2],\n",
       " [2018, 7, 23, 3],\n",
       " [2018, 7, 24, 4],\n",
       " [2018, 7, 25, 5],\n",
       " [2018, 7, 26, 6],\n",
       " [2018, 7, 27, 0],\n",
       " [2018, 7, 28, 1],\n",
       " [2018, 7, 29, 2],\n",
       " [2018, 7, 30, 3],\n",
       " [2018, 7, 31, 4],\n",
       " [2018, 8, 1, 5],\n",
       " [2018, 8, 2, 6],\n",
       " [2018, 8, 3, 0],\n",
       " [2018, 8, 4, 1],\n",
       " [2018, 8, 5, 2],\n",
       " [2018, 8, 6, 3],\n",
       " [2018, 8, 7, 4],\n",
       " [2018, 8, 8, 5],\n",
       " [2018, 8, 9, 6],\n",
       " [2018, 8, 10, 0],\n",
       " [2018, 8, 11, 1],\n",
       " [2018, 8, 12, 2],\n",
       " [2018, 8, 13, 3],\n",
       " [2018, 8, 14, 4],\n",
       " [2018, 8, 15, 5],\n",
       " [2018, 8, 16, 6],\n",
       " [2018, 8, 17, 0],\n",
       " [2018, 8, 18, 1],\n",
       " [2018, 8, 19, 2],\n",
       " [2018, 8, 20, 3],\n",
       " [2018, 8, 21, 4],\n",
       " [2018, 8, 22, 5],\n",
       " [2018, 8, 23, 6],\n",
       " [2018, 8, 24, 0],\n",
       " [2018, 8, 25, 1],\n",
       " [2018, 8, 26, 2],\n",
       " [2018, 8, 27, 3],\n",
       " [2018, 8, 28, 4],\n",
       " [2018, 8, 29, 5],\n",
       " [2018, 8, 30, 6],\n",
       " [2018, 8, 31, 0],\n",
       " [2018, 9, 1, 1],\n",
       " [2018, 9, 2, 2],\n",
       " [2018, 9, 3, 3],\n",
       " [2018, 9, 4, 4],\n",
       " [2018, 9, 5, 5],\n",
       " [2018, 9, 6, 6],\n",
       " [2018, 9, 7, 0],\n",
       " [2018, 9, 8, 1],\n",
       " [2018, 9, 9, 2],\n",
       " [2018, 9, 10, 3],\n",
       " [2018, 9, 11, 4],\n",
       " [2018, 9, 12, 5],\n",
       " [2018, 9, 13, 6],\n",
       " [2018, 9, 14, 0],\n",
       " [2018, 9, 15, 1],\n",
       " [2018, 9, 16, 2],\n",
       " [2018, 9, 17, 3],\n",
       " [2018, 9, 18, 4],\n",
       " [2018, 9, 19, 5],\n",
       " [2018, 9, 20, 6],\n",
       " [2018, 9, 21, 0],\n",
       " [2018, 9, 22, 1],\n",
       " [2018, 9, 23, 2],\n",
       " [2018, 9, 24, 3],\n",
       " [2018, 9, 25, 4],\n",
       " [2018, 9, 26, 5]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dates[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dates = np.asarray(x_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1733, 4)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day_map = {} \n",
    "for i,d in enumerate(data['요일'].unique()):\n",
    "    week_day_map[d] = i \n",
    "    \n",
    "data['요일'] = data['요일'].map(week_day_map) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = data.iloc[:,1:].max(0) \n",
    "data.iloc[:,1:] = data.iloc[:,1:] / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>요일</th>\n",
       "      <th>배추_거래량(kg)</th>\n",
       "      <th>배추_가격(원/kg)</th>\n",
       "      <th>무_거래량(kg)</th>\n",
       "      <th>무_가격(원/kg)</th>\n",
       "      <th>양파_거래량(kg)</th>\n",
       "      <th>양파_가격(원/kg)</th>\n",
       "      <th>건고추_거래량(kg)</th>\n",
       "      <th>건고추_가격(원/kg)</th>\n",
       "      <th>...</th>\n",
       "      <th>청상추_거래량(kg)</th>\n",
       "      <th>청상추_가격(원/kg)</th>\n",
       "      <th>백다다기_거래량(kg)</th>\n",
       "      <th>백다다기_가격(원/kg)</th>\n",
       "      <th>애호박_거래량(kg)</th>\n",
       "      <th>애호박_가격(원/kg)</th>\n",
       "      <th>캠벨얼리_거래량(kg)</th>\n",
       "      <th>캠벨얼리_가격(원/kg)</th>\n",
       "      <th>샤인마스캇_거래량(kg)</th>\n",
       "      <th>샤인마스캇_가격(원/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.015520</td>\n",
       "      <td>0.0658</td>\n",
       "      <td>0.020797</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.024437</td>\n",
       "      <td>0.6405</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.064389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.511068</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.398376</td>\n",
       "      <td>0.023032</td>\n",
       "      <td>0.356152</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.08056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.273068</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.440354</td>\n",
       "      <td>0.280882</td>\n",
       "      <td>0.460735</td>\n",
       "      <td>0.6175</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.026130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457792</td>\n",
       "      <td>0.422302</td>\n",
       "      <td>0.227365</td>\n",
       "      <td>0.386475</td>\n",
       "      <td>0.745976</td>\n",
       "      <td>0.297728</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.15540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.224029</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.368802</td>\n",
       "      <td>0.310294</td>\n",
       "      <td>0.416530</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.025416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387559</td>\n",
       "      <td>0.383287</td>\n",
       "      <td>0.067041</td>\n",
       "      <td>0.428410</td>\n",
       "      <td>0.278846</td>\n",
       "      <td>0.321334</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>0.11412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        요일  배추_거래량(kg)  배추_가격(원/kg)  무_거래량(kg)  무_가격(원/kg)  \\\n",
       "0  2016-01-01  0.000000    0.000000       0.0000   0.000000    0.000000   \n",
       "1  2016-01-02  0.166667    0.015520       0.0658   0.020797    0.264706   \n",
       "2  2016-01-03  0.333333    0.000000       0.0000   0.000000    0.000000   \n",
       "3  2016-01-04  0.500000    0.273068       0.0956   0.440354    0.280882   \n",
       "4  2016-01-05  0.666667    0.224029       0.0884   0.368802    0.310294   \n",
       "\n",
       "   양파_거래량(kg)  양파_가격(원/kg)  건고추_거래량(kg)  건고추_가격(원/kg)  ...  청상추_거래량(kg)  \\\n",
       "0    0.000000       0.0000     0.000000      0.000000  ...     0.000000   \n",
       "1    0.024437       0.6405     0.000007      0.064389  ...     0.060900   \n",
       "2    0.000000       0.0000     0.000000      0.000000  ...     0.000000   \n",
       "3    0.460735       0.6175     0.001690      0.026130  ...     0.457792   \n",
       "4    0.416530       0.6065     0.002690      0.025416  ...     0.387559   \n",
       "\n",
       "   청상추_가격(원/kg)  백다다기_거래량(kg)  백다다기_가격(원/kg)  애호박_거래량(kg)  애호박_가격(원/kg)  \\\n",
       "0      0.000000      0.000000       0.000000     0.000000      0.000000   \n",
       "1      0.511068      0.000197       0.398376     0.023032      0.356152   \n",
       "2      0.000000      0.000000       0.000000     0.000000      0.000000   \n",
       "3      0.422302      0.227365       0.386475     0.745976      0.297728   \n",
       "4      0.383287      0.067041       0.428410     0.278846      0.321334   \n",
       "\n",
       "   캠벨얼리_거래량(kg)  캠벨얼리_가격(원/kg)  샤인마스캇_거래량(kg)  샤인마스캇_가격(원/kg)  \n",
       "0      0.000000        0.00000            0.0             0.0  \n",
       "1      0.000590        0.08056            0.0             0.0  \n",
       "2      0.000000        0.00000            0.0             0.0  \n",
       "3      0.001814        0.15540            0.0             0.0  \n",
       "4      0.005911        0.11412            0.0             0.0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 28\n",
    "future_size = 28    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1677/1677 [00:01<00:00, 1277.11it/s]\n"
     ]
    }
   ],
   "source": [
    "enc_inputs = [] \n",
    "dec_inputs = [] \n",
    "targets = [] \n",
    "enc_marks = [] \n",
    "dec_marks = [] \n",
    "\n",
    "for i in tqdm(range(data.shape[0]-window_size-future_size)): \n",
    "    x = data.iloc[i:i+window_size, 2:].to_numpy() \n",
    "    start_token = data.iloc[i:i+window_size, 3::2].to_numpy() \n",
    "    y = data.iloc[i+window_size:i+window_size+future_size, 3::2].to_numpy()  \n",
    "    y_0 = np.zeros((28,21))\n",
    "    enc_inputs.append(x) \n",
    "    dec_inputs.append(np.concatenate([start_token,y_0]))\n",
    "    targets.append(y)\n",
    "\n",
    "    enc_marks.append(x_dates[i:i+window_size]) \n",
    "    dec_marks.append(x_dates[i:i+window_size+future_size])\n",
    "    \n",
    "enc_inputs = np.asarray(enc_inputs) \n",
    "dec_inputs = np.asarray(dec_inputs) \n",
    "targets = np.asarray(targets) \n",
    "enc_marks = np.asarray(enc_marks) \n",
    "dec_marks = np.asarray(dec_marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1509, 28, 42),\n",
       " (168, 28, 42),\n",
       " (1509, 56, 21),\n",
       " (168, 56, 21),\n",
       " (1509, 28, 21),\n",
       " (168, 28, 21),\n",
       " (1509, 28, 4),\n",
       " (168, 28, 4),\n",
       " (1509, 56, 4),\n",
       " (168, 56, 4))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_enc_inputs, val_enc_inputs, train_dec_inputs, val_dec_inputs = train_test_split(enc_inputs, \n",
    "                                                                                      dec_inputs, \n",
    "                                                                                      random_state = 888, \n",
    "                                                                                      test_size = 0.1)  \n",
    "\n",
    "train_targets, val_targets, _, _ = train_test_split(targets, \n",
    "                                                    dec_inputs, \n",
    "                                                    random_state = 888, \n",
    "                                                    test_size = 0.1) \n",
    "\n",
    "\n",
    "train_enc_marks, val_enc_marks, _, _ = train_test_split(enc_marks, \n",
    "                                                        dec_inputs, \n",
    "                                                        random_state = 888, \n",
    "                                                        test_size = 0.1)\n",
    "\n",
    "train_dec_marks, val_dec_marks, _, _ = train_test_split(dec_marks, \n",
    "                                                        dec_inputs, \n",
    "                                                        random_state = 888, \n",
    "                                                        test_size = 0.1)\n",
    "\n",
    "\n",
    "train_enc_inputs.shape, val_enc_inputs.shape, train_dec_inputs.shape, val_dec_inputs.shape, train_targets.shape, val_targets.shape, train_enc_marks.shape, val_enc_marks.shape, train_dec_marks.shape, val_dec_marks.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, encoder_input, decoder_input, target_input, encoder_marks, decoder_marks): \n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input \n",
    "        self.target_input = target_input  \n",
    "        self.encoder_marks = encoder_marks \n",
    "        self.decoder_marks = decoder_marks \n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.encoder_input) \n",
    "    \n",
    "    def __getitem__(self, i):  \n",
    "        return {\n",
    "            'encoder_input': torch.tensor(self.encoder_input[i], dtype=torch.float32), \n",
    "            'decoder_input': torch.tensor(self.decoder_input[i], dtype=torch.float32), \n",
    "            'target': torch.tensor(self.target_input[i], dtype=torch.float32),  \n",
    "            'encoder_marks': torch.tensor(self.encoder_marks[i], dtype=torch.float32),  \n",
    "            'decoder_marks': torch.tensor(self.decoder_marks[i], dtype=torch.float32) \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = CustomDataset(train_enc_inputs, train_dec_inputs, train_targets, train_enc_marks, train_dec_marks)\n",
    "val_dataset = CustomDataset(val_enc_inputs, val_dec_inputs, val_targets, val_enc_marks, val_dec_marks)  \n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True) \n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 28, 42]), torch.Size([32, 56, 21]), torch.Size([32, 28, 21]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(train_dataloader)) \n",
    "\n",
    "sample_batch['encoder_input'].shape, sample_batch['decoder_input'].shape, sample_batch['target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 28, 4]), torch.Size([32, 56, 4]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch['encoder_marks'].shape, sample_batch['decoder_marks'].shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_metric(pred, true): \n",
    "    pred = pred[:, [6,13,27]] \n",
    "    true = true[:, [6,13,27]] \n",
    "    target = torch.where(true!=0) \n",
    "    true = true[target] \n",
    "    pred = pred[target] \n",
    "    score = torch.mean(torch.abs((true-pred)/true)) \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to system path \n",
    "import sys\n",
    "if not 'Informer2020' in sys.path:\n",
    "    sys.path += ['Informer2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InformerStack(\n",
       "  (enc_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(42, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TemporalEmbedding(\n",
       "      (weekday_embed): FixedEmbedding(\n",
       "        (emb): Embedding(7, 512)\n",
       "      )\n",
       "      (day_embed): FixedEmbedding(\n",
       "        (emb): Embedding(32, 512)\n",
       "      )\n",
       "      (month_embed): FixedEmbedding(\n",
       "        (emb): Embedding(13, 512)\n",
       "      )\n",
       "      (year_embed): FixedEmbedding(\n",
       "        (emb): Embedding(2022, 512)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (dec_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(21, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TemporalEmbedding(\n",
       "      (weekday_embed): FixedEmbedding(\n",
       "        (emb): Embedding(7, 512)\n",
       "      )\n",
       "      (day_embed): FixedEmbedding(\n",
       "        (emb): Embedding(32, 512)\n",
       "      )\n",
       "      (month_embed): FixedEmbedding(\n",
       "        (emb): Embedding(13, 512)\n",
       "      )\n",
       "      (year_embed): FixedEmbedding(\n",
       "        (emb): Embedding(2022, 512)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): EncoderStack(\n",
       "    (encoders): ModuleList(\n",
       "      (0): Encoder(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (2): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): ConvLayer(\n",
       "            (downConv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): ELU(alpha=1.0)\n",
       "            (maxPool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "          (1): ConvLayer(\n",
       "            (downConv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): ELU(alpha=1.0)\n",
       "            (maxPool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Encoder(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): ConvLayer(\n",
       "            (downConv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): ELU(alpha=1.0)\n",
       "            (maxPool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): Encoder(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (conv_layers): ModuleList()\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projection): Linear(in_features=512, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Informer2020.models.model import Informer, InformerStack\n",
    "\n",
    "model = InformerStack(enc_in = 42, \n",
    "                      dec_in = 21, \n",
    "                      c_out = 21, \n",
    "                      seq_len = 28, \n",
    "                      label_len = 28, \n",
    "                      out_len = 28, \n",
    "                      freq = 'd') \n",
    "model.cuda()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4) \n",
    "criterion = nn.L1Loss() # mae \n",
    "custom_metric = my_custom_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    # 반올림\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # hh:mm:ss으로 형태 변경\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.3468509539961815\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.25087886974215506\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.21035724033912023\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.18763488288968802\n",
      "\n",
      "  Average training loss: 0.17581281314293543\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.14027662575244904\n",
      "  Validation epoch took: 0:00:00\n",
      "Saving Best Checkpoint....\n",
      "\n",
      "======== Epoch 2 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.11379319801926613\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.11049469523131847\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.10970126365621885\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.10893464833498001\n",
      "\n",
      "  Average training loss: 0.10876298835501075\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.12354374056061108\n",
      "  Validation epoch took: 0:00:00\n",
      "Saving Best Checkpoint....\n",
      "\n",
      "======== Epoch 3 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.10598200634121895\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.10402954742312431\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.10272005920608838\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.10256036669015885\n",
      "\n",
      "  Average training loss: 0.10259843058884144\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.11913131425778072\n",
      "  Validation epoch took: 0:00:00\n",
      "Saving Best Checkpoint....\n",
      "\n",
      "======== Epoch 4 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.09881931617856025\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.10139314010739327\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.10254260152578354\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.1021287951618433\n",
      "\n",
      "  Average training loss: 0.10218715171019237\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.12147746110955875\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 5 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.10088991597294808\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.09872333817183972\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.09787939886252085\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.09754335023462772\n",
      "\n",
      "  Average training loss: 0.09734471359600623\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.11358022441466649\n",
      "  Validation epoch took: 0:00:00\n",
      "Saving Best Checkpoint....\n",
      "\n",
      "======== Epoch 6 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.09402331560850144\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.09433434531092644\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.09479808285832406\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.09438203256577253\n",
      "\n",
      "  Average training loss: 0.0932798918026189\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.12528718262910843\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 7 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.09066745340824127\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.09143518172204494\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.09115085850159327\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.0910371582955122\n",
      "\n",
      "  Average training loss: 0.09146302255491416\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.11342945819099744\n",
      "  Validation epoch took: 0:00:00\n",
      "Saving Best Checkpoint....\n",
      "\n",
      "======== Epoch 8 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.09552487134933471\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.09324498474597931\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.09280712828040123\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.09213743731379509\n",
      "\n",
      "  Average training loss: 0.09177655431752403\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.10860181599855423\n",
      "  Validation epoch took: 0:00:00\n",
      "Saving Best Checkpoint....\n",
      "\n",
      "======== Epoch 9 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.09029426723718643\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.09094601385295391\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.09052616357803345\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.08988982792943716\n",
      "\n",
      "  Average training loss: 0.0891733927031358\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.10075762743751208\n",
      "  Validation epoch took: 0:00:00\n",
      "Saving Best Checkpoint....\n",
      "\n",
      "======== Epoch 10 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08851629421114922\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08655139058828354\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08693151200811068\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.0869498796761036\n",
      "\n",
      "  Average training loss: 0.08673673573260506\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.10932900508244832\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 11 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08804235756397247\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08678647093474864\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08708917970458667\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.08738396428525448\n",
      "\n",
      "  Average training loss: 0.08679680557300647\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.12152684728304546\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 12 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08729680702090263\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.0867539495229721\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08624666631221771\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08543648198246956\n",
      "\n",
      "  Average training loss: 0.085192178376019\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.1048165646692117\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 13 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08199575394392014\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08368419483304024\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.0844863976041476\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.08374590780586004\n",
      "\n",
      "  Average training loss: 0.08391748151431482\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.10892329861720403\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 14 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08918980211019516\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08673132956027985\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.0837522474428018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.08339553605765104\n",
      "\n",
      "  Average training loss: 0.08286292074869077\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.11673355102539062\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 15 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08392135500907898\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08207702338695526\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08191662455598513\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08131183870136738\n",
      "\n",
      "  Average training loss: 0.0812752628698945\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.10015162204702695\n",
      "  Validation epoch took: 0:00:00\n",
      "Saving Best Checkpoint....\n",
      "\n",
      "======== Epoch 16 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.07920826971530914\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.0804392572492361\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08045971567432085\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08050343934446573\n",
      "\n",
      "  Average training loss: 0.08098326902836561\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.11138978103796641\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 17 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08533208891749382\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08826828673481942\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08835094372431437\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.08778954837471246\n",
      "\n",
      "  Average training loss: 0.08750257501378655\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.13333478569984436\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 18 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08896773010492325\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08647161051630974\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08582359949747721\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08600705396384001\n",
      "\n",
      "  Average training loss: 0.08615106173480551\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.13779809325933456\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 19 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08850906193256378\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08751283250749112\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08627207254370053\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08549676463007927\n",
      "\n",
      "  Average training loss: 0.08545868331566453\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.1291575481494268\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 20 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08622895926237106\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08411406688392162\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08312489787737529\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.08296406120061875\n",
      "\n",
      "  Average training loss: 0.083508784417063\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.13318909456332526\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 21 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08371125459671021\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08166761621832848\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.0813709077735742\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.08154134266078472\n",
      "\n",
      "  Average training loss: 0.08148383519922693\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.11911619702974956\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 22 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08186759129166603\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08085024021565915\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.07985013822714487\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.08014144469052553\n",
      "\n",
      "  Average training loss: 0.0800921890574197\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.12324611345926921\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 23 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08227314502000808\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08205332309007644\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08107009033362071\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.08065108321607113\n",
      "\n",
      "  Average training loss: 0.08076972793787718\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.11241526032487552\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 24 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08117886781692504\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08120699636638165\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08130752344926198\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.08005145844072104\n",
      "\n",
      "  Average training loss: 0.08011847191179793\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.11833496888478597\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 25 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08276324495673179\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08114096708595753\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.0805381752550602\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.07976392414420844\n",
      "\n",
      "  Average training loss: 0.07921797689050436\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.1159827932715416\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 26 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08088825196027756\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.07925374023616313\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.07941585058967272\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.07940336186438798\n",
      "\n",
      "  Average training loss: 0.07973585138097405\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.13886898756027222\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 27 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.08037798330187798\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.08077703602612019\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.08041481251517932\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.0798515910282731\n",
      "\n",
      "  Average training loss: 0.07943043345585465\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.12822685142358145\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 28 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = 0.07982805147767066\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = 0.0792696300894022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = 0.07910440390308698\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = 0.0788056120276451\n",
      "\n",
      "  Average training loss: 0.07859769851590197\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: 0.11327376713355382\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 29 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 30 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 31 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 32 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 33 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 34 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 35 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 36 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 37 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 38 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 39 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 40 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 41 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 42 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 43 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 44 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 45 / 50 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 46 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 47 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 48 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 49 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:04.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "======== Epoch 50 / 50 ========\n",
      "Training...\n",
      "  Batch    10  of     48.    Elapsed: 0:00:01.\n",
      "  current average loss = nan\n",
      "  Batch    20  of     48.    Elapsed: 0:00:02.\n",
      "  current average loss = nan\n",
      "  Batch    30  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "  Batch    40  of     48.    Elapsed: 0:00:03.\n",
      "  current average loss = nan\n",
      "\n",
      "  Average training loss: nan\n",
      "  Training epoch took: 0:00:04\n",
      "\n",
      "Running Validation...\n",
      "\n",
      "  Average validation loss: nan\n",
      "  Validation epoch took: 0:00:00\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "val_losses = [] \n",
    "epochs = 50 \n",
    "model.zero_grad() \n",
    "\n",
    "# for reproducibility \n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "for epoch_i in range(0, epochs): \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time() \n",
    "    total_loss = 0 \n",
    "    model.train() \n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):  \n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print('  current average loss = {}'.format(total_loss / step))\n",
    "            \n",
    "        encoder_input = batch['encoder_input'].to(device) \n",
    "        decoder_input = batch['decoder_input'].to(device) \n",
    "        target = batch['target'].to(device) \n",
    "        enc_marks = batch['encoder_marks'].to(device)  \n",
    "        dec_marks = batch['decoder_marks'].to(device) \n",
    "        \n",
    "        with torch.cuda.amp.autocast(): \n",
    "            output = model(x_enc=encoder_input,\n",
    "                           x_mark_enc=enc_marks, \n",
    "                           x_dec=decoder_input, \n",
    "                           x_mark_dec=dec_marks) \n",
    "            loss = criterion(output, target) \n",
    "            total_loss += loss.item() \n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step() \n",
    "            # gradient initialization \n",
    "            model.zero_grad() \n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader) \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time() \n",
    "    model.eval() \n",
    "    eval_loss = 0 \n",
    "    for batch in val_dataloader: \n",
    "        encoder_input = batch['encoder_input'].to(device) \n",
    "        decoder_input = batch['decoder_input'].to(device) \n",
    "        target = batch['target'].to(device) \n",
    "        enc_marks = batch['encoder_marks'].to(device)  \n",
    "        dec_marks = batch['decoder_marks'].to(device)   \n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            outputs = model(x_enc=encoder_input,\n",
    "                            x_mark_enc=enc_marks, \n",
    "                            x_dec=decoder_input, \n",
    "                            x_mark_dec=dec_marks) \n",
    "            loss = criterion(outputs, target) \n",
    "            eval_loss += loss.item()  \n",
    "        \n",
    "    avg_val_loss = eval_loss / len(val_dataloader) \n",
    "    print(\"\")\n",
    "    print(\"  Average validation loss: {}\".format(avg_val_loss))\n",
    "    print(\"  Validation epoch took: {:}\".format(format_time(time.time() - t0))) \n",
    "    \n",
    "    val_losses.append(avg_val_loss) \n",
    "    \n",
    "    if np.min(val_losses) == val_losses[-1]: \n",
    "        print(\"Saving Best Checkpoint....\")\n",
    "        torch.save(model.state_dict(), \"INFORMER_\" + str(epoch_i + 1)) \n",
    "\n",
    "        \n",
    "        \n",
    "print(\"\")\n",
    "print(\"Training Complete!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InformerStack(\n",
       "  (enc_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(42, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TemporalEmbedding(\n",
       "      (weekday_embed): FixedEmbedding(\n",
       "        (emb): Embedding(7, 512)\n",
       "      )\n",
       "      (day_embed): FixedEmbedding(\n",
       "        (emb): Embedding(32, 512)\n",
       "      )\n",
       "      (month_embed): FixedEmbedding(\n",
       "        (emb): Embedding(13, 512)\n",
       "      )\n",
       "      (year_embed): FixedEmbedding(\n",
       "        (emb): Embedding(2022, 512)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (dec_embedding): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(21, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TemporalEmbedding(\n",
       "      (weekday_embed): FixedEmbedding(\n",
       "        (emb): Embedding(7, 512)\n",
       "      )\n",
       "      (day_embed): FixedEmbedding(\n",
       "        (emb): Embedding(32, 512)\n",
       "      )\n",
       "      (month_embed): FixedEmbedding(\n",
       "        (emb): Embedding(13, 512)\n",
       "      )\n",
       "      (year_embed): FixedEmbedding(\n",
       "        (emb): Embedding(2022, 512)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): EncoderStack(\n",
       "    (encoders): ModuleList(\n",
       "      (0): Encoder(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (2): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): ConvLayer(\n",
       "            (downConv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): ELU(alpha=1.0)\n",
       "            (maxPool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "          (1): ConvLayer(\n",
       "            (downConv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): ELU(alpha=1.0)\n",
       "            (maxPool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Encoder(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (1): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): ConvLayer(\n",
       "            (downConv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): ELU(alpha=1.0)\n",
       "            (maxPool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): Encoder(\n",
       "        (attn_layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (attention): AttentionLayer(\n",
       "              (inner_attention): ProbAttention(\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (conv_layers): ModuleList()\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attention): AttentionLayer(\n",
       "          (inner_attention): ProbAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (cross_attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projection): Linear(in_features=512, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('INFORMER_15') \n",
    "test_model = InformerStack(enc_in = 42, \n",
    "                           dec_in = 21, \n",
    "                           c_out = 21, \n",
    "                           seq_len = 28, \n",
    "                           label_len = 28, \n",
    "                           out_len = 28, \n",
    "                           freq = 'd') \n",
    "test_model.load_state_dict(checkpoint) \n",
    "test_model.eval() \n",
    "test_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "public_date_list = submission[submission['예측대상일자'].str.contains('2020')]['예측대상일자'].str.split('+').str[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'금요일': 0, '토요일': 1, '일요일': 2, '월요일': 3, '화요일': 4, '수요일': 5, '목요일': 6}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the same weekday map as the training set \n",
    "week_day_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a naive version for testing \n",
    "def get_decoder_marks(dates):\n",
    "    last_date = dates[-1] \n",
    "    cur_date = last_date \n",
    "    next_dates = [] \n",
    "    for i in range(28): \n",
    "        next_day = cur_date[2] + 1  \n",
    "        next_month = cur_date[1]\n",
    "        next_year = cur_date[0] \n",
    "\n",
    "        if next_day == 31 and cur_date[1] in [4,6,9,11]:  \n",
    "            next_day = 1 \n",
    "            next_month = cur_date[1] + 1   \n",
    "        elif next_day == 32 and cur_date[1] in [1,3,5,7,8,10,12]:  \n",
    "            next_day = 1 \n",
    "            next_month = cur_date[1] + 1 \n",
    "            \n",
    "        next_weekday = cur_date[3] + 1 \n",
    "        if next_weekday > 6: \n",
    "            next_weekday = 0 \n",
    "            \n",
    "        final_date = [next_year, next_month, next_day, next_weekday] \n",
    "        cur_date = final_date\n",
    "        \n",
    "        next_dates.append(final_date)  \n",
    "    \n",
    "    return np.asarray(next_dates)  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:01<00:00, 22.29it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = [] \n",
    "torch_norm = torch.tensor(norm.to_numpy()[2::2]) \n",
    "\n",
    "test_model.eval() \n",
    "\n",
    "# change to eval mode \n",
    "for date in tqdm(public_date_list):\n",
    "    test_df = pd.read_csv(f'./public_data/test_files/test_{date}.csv')\n",
    "    data = pd.read_csv('./public_data/train.csv')\n",
    "    data = pd.concat([data, test_df]).iloc[-window_size:]\n",
    "    \n",
    "    date_info = data['date'].values \n",
    "    weekday_info = data['요일'].values\n",
    "    data['요일'] = data['요일'].map(week_day_map) \n",
    "        \n",
    "    weekdays = [] \n",
    "    for i in range(len(weekday_info)): \n",
    "        weekdays.append(week_day_map[weekday_info[i]]) \n",
    "    \n",
    "    x_test_dates = [] \n",
    "\n",
    "    for i in range(len(date_info)): \n",
    "        d = date_info[i].split('-') \n",
    "        x_date = [] \n",
    "        x_date.append(int(d[0])) \n",
    "        x_date.append(int(d[1])) \n",
    "        x_date.append(int(d[2])) \n",
    "        x_date.append(int(weekdays[i])) \n",
    "        x_test_dates.append(x_date) \n",
    "        \n",
    "        \n",
    "    data = data.iloc[:,1:] / norm  \n",
    "\n",
    "    \n",
    "    # get encoder_input, decoder_input, encoder_marks, decoder_marks\n",
    "    test_encoder_inputs = data.iloc[:, 1:].to_numpy() \n",
    "    test_decoder_inputs = data.iloc[:, 2::2].to_numpy() \n",
    "    target_tokens = np.zeros((28,21)) \n",
    "    test_decoder_inputs = np.concatenate([test_decoder_inputs,target_tokens])\n",
    "    \n",
    "    test_encoder_marks = np.asarray(x_test_dates) \n",
    "    test_decoder_marks = get_decoder_marks(x_test_dates) \n",
    "    test_decoder_marks = np.concatenate([x_test_dates, test_decoder_marks]) \n",
    "    \n",
    "    \n",
    "    # convert to torch tensors and reshape \n",
    "    test_encoder_inputs = torch.tensor(test_encoder_inputs, dtype=torch.float32)\n",
    "    test_encoder_inputs = torch.reshape(test_encoder_inputs, (-1,test_encoder_inputs.shape[0], test_encoder_inputs.shape[1])) \n",
    "    test_encoder_inputs = test_encoder_inputs.to(device)\n",
    "    \n",
    "    test_decoder_inputs = torch.tensor(test_decoder_inputs, dtype=torch.float32)\n",
    "    test_decoder_inputs = torch.reshape(test_decoder_inputs, (-1,test_decoder_inputs.shape[0], test_decoder_inputs.shape[1]))\n",
    "    test_decoder_inputs = test_decoder_inputs.to(device) \n",
    "    \n",
    "    test_encoder_marks = torch.tensor(test_encoder_marks, dtype=torch.float32)\n",
    "    test_encoder_marks = torch.reshape(test_encoder_marks, (-1,test_encoder_marks.shape[0],test_encoder_marks.shape[1]))\n",
    "    test_encoder_marks = test_encoder_marks.to(device) \n",
    "    \n",
    "    test_decoder_marks = torch.tensor(test_decoder_marks, dtype=torch.float32) \n",
    "    test_decoder_marks = torch.reshape(test_decoder_marks, (-1,test_decoder_marks.shape[0],test_decoder_marks.shape[1]))\n",
    "    test_decoder_marks = test_decoder_marks.to(device)  \n",
    "    \n",
    "        \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        outputs = test_model(x_enc=test_encoder_inputs,\n",
    "                             x_mark_enc=test_encoder_marks, \n",
    "                             x_dec=test_decoder_inputs, \n",
    "                             x_mark_dec=test_decoder_marks) \n",
    "    \n",
    "    outputs = outputs.cpu()\n",
    "\n",
    "    outputs = outputs*torch_norm  \n",
    "        \n",
    "    \n",
    "    idx = submission[submission['예측대상일자'].str.contains(date)].index\n",
    "    submission.loc[idx, '배추_가격(원/kg)':] = outputs[0,[6,13,27]].numpy()   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>예측대상일자</th>\n",
       "      <th>배추_가격(원/kg)</th>\n",
       "      <th>무_가격(원/kg)</th>\n",
       "      <th>양파_가격(원/kg)</th>\n",
       "      <th>건고추_가격(원/kg)</th>\n",
       "      <th>마늘_가격(원/kg)</th>\n",
       "      <th>대파_가격(원/kg)</th>\n",
       "      <th>얼갈이배추_가격(원/kg)</th>\n",
       "      <th>양배추_가격(원/kg)</th>\n",
       "      <th>깻잎_가격(원/kg)</th>\n",
       "      <th>...</th>\n",
       "      <th>당근_가격(원/kg)</th>\n",
       "      <th>파프리카_가격(원/kg)</th>\n",
       "      <th>새송이_가격(원/kg)</th>\n",
       "      <th>팽이버섯_가격(원/kg)</th>\n",
       "      <th>토마토_가격(원/kg)</th>\n",
       "      <th>청상추_가격(원/kg)</th>\n",
       "      <th>백다다기_가격(원/kg)</th>\n",
       "      <th>애호박_가격(원/kg)</th>\n",
       "      <th>캠벨얼리_가격(원/kg)</th>\n",
       "      <th>샤인마스캇_가격(원/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-29+1week</td>\n",
       "      <td>847.736299</td>\n",
       "      <td>622.958150</td>\n",
       "      <td>763.107955</td>\n",
       "      <td>17767.369491</td>\n",
       "      <td>4726.961725</td>\n",
       "      <td>1937.300964</td>\n",
       "      <td>874.740040</td>\n",
       "      <td>790.082339</td>\n",
       "      <td>2827.664690</td>\n",
       "      <td>...</td>\n",
       "      <td>1543.306187</td>\n",
       "      <td>3657.336922</td>\n",
       "      <td>1653.562665</td>\n",
       "      <td>1558.366600</td>\n",
       "      <td>1978.029460</td>\n",
       "      <td>2268.147852</td>\n",
       "      <td>1975.494076</td>\n",
       "      <td>1098.205279</td>\n",
       "      <td>2117.845975</td>\n",
       "      <td>7496.036291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-29+2week</td>\n",
       "      <td>872.360468</td>\n",
       "      <td>748.475204</td>\n",
       "      <td>754.129648</td>\n",
       "      <td>15041.441629</td>\n",
       "      <td>4781.794223</td>\n",
       "      <td>2094.085039</td>\n",
       "      <td>942.172809</td>\n",
       "      <td>970.965916</td>\n",
       "      <td>4476.921110</td>\n",
       "      <td>...</td>\n",
       "      <td>1677.199602</td>\n",
       "      <td>4038.487540</td>\n",
       "      <td>1978.420615</td>\n",
       "      <td>1855.822767</td>\n",
       "      <td>2814.846479</td>\n",
       "      <td>3660.647615</td>\n",
       "      <td>2269.150782</td>\n",
       "      <td>1537.469433</td>\n",
       "      <td>2350.755967</td>\n",
       "      <td>3933.650255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-29+4week</td>\n",
       "      <td>1012.851000</td>\n",
       "      <td>1001.380391</td>\n",
       "      <td>813.972235</td>\n",
       "      <td>18219.587242</td>\n",
       "      <td>5275.259938</td>\n",
       "      <td>2181.421726</td>\n",
       "      <td>1390.371976</td>\n",
       "      <td>1216.404835</td>\n",
       "      <td>8324.011843</td>\n",
       "      <td>...</td>\n",
       "      <td>1933.192983</td>\n",
       "      <td>5436.511889</td>\n",
       "      <td>2598.682880</td>\n",
       "      <td>2344.333830</td>\n",
       "      <td>4154.447987</td>\n",
       "      <td>5233.349069</td>\n",
       "      <td>2469.133577</td>\n",
       "      <td>2736.994448</td>\n",
       "      <td>3101.043589</td>\n",
       "      <td>1223.930791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-30+1week</td>\n",
       "      <td>809.706300</td>\n",
       "      <td>609.735913</td>\n",
       "      <td>773.450613</td>\n",
       "      <td>16867.078315</td>\n",
       "      <td>4738.021598</td>\n",
       "      <td>1889.236635</td>\n",
       "      <td>830.138902</td>\n",
       "      <td>821.702757</td>\n",
       "      <td>2283.292237</td>\n",
       "      <td>...</td>\n",
       "      <td>1511.206403</td>\n",
       "      <td>3355.762516</td>\n",
       "      <td>1605.403304</td>\n",
       "      <td>1519.446560</td>\n",
       "      <td>1881.702787</td>\n",
       "      <td>1525.904181</td>\n",
       "      <td>1857.848692</td>\n",
       "      <td>907.637639</td>\n",
       "      <td>2023.351006</td>\n",
       "      <td>7934.085131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-30+2week</td>\n",
       "      <td>891.161263</td>\n",
       "      <td>729.734731</td>\n",
       "      <td>767.378688</td>\n",
       "      <td>13982.647804</td>\n",
       "      <td>4782.371974</td>\n",
       "      <td>2049.027422</td>\n",
       "      <td>934.258050</td>\n",
       "      <td>1011.369930</td>\n",
       "      <td>4413.143608</td>\n",
       "      <td>...</td>\n",
       "      <td>1654.286608</td>\n",
       "      <td>3715.861926</td>\n",
       "      <td>1980.861783</td>\n",
       "      <td>1870.338272</td>\n",
       "      <td>2700.905111</td>\n",
       "      <td>3171.180625</td>\n",
       "      <td>2171.252013</td>\n",
       "      <td>1438.130309</td>\n",
       "      <td>2161.055803</td>\n",
       "      <td>3901.496530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2021-11-03+2week</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2021-11-03+4week</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2021-11-04+1week</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2021-11-04+2week</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2021-11-04+4week</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               예측대상일자  배추_가격(원/kg)   무_가격(원/kg)  양파_가격(원/kg)  건고추_가격(원/kg)  \\\n",
       "0    2020-09-29+1week   847.736299   622.958150   763.107955  17767.369491   \n",
       "1    2020-09-29+2week   872.360468   748.475204   754.129648  15041.441629   \n",
       "2    2020-09-29+4week  1012.851000  1001.380391   813.972235  18219.587242   \n",
       "3    2020-09-30+1week   809.706300   609.735913   773.450613  16867.078315   \n",
       "4    2020-09-30+2week   891.161263   729.734731   767.378688  13982.647804   \n",
       "..                ...          ...          ...          ...           ...   \n",
       "223  2021-11-03+2week     0.000000     0.000000     0.000000      0.000000   \n",
       "224  2021-11-03+4week     0.000000     0.000000     0.000000      0.000000   \n",
       "225  2021-11-04+1week     0.000000     0.000000     0.000000      0.000000   \n",
       "226  2021-11-04+2week     0.000000     0.000000     0.000000      0.000000   \n",
       "227  2021-11-04+4week     0.000000     0.000000     0.000000      0.000000   \n",
       "\n",
       "     마늘_가격(원/kg)  대파_가격(원/kg)  얼갈이배추_가격(원/kg)  양배추_가격(원/kg)  깻잎_가격(원/kg)  ...  \\\n",
       "0    4726.961725  1937.300964      874.740040    790.082339  2827.664690  ...   \n",
       "1    4781.794223  2094.085039      942.172809    970.965916  4476.921110  ...   \n",
       "2    5275.259938  2181.421726     1390.371976   1216.404835  8324.011843  ...   \n",
       "3    4738.021598  1889.236635      830.138902    821.702757  2283.292237  ...   \n",
       "4    4782.371974  2049.027422      934.258050   1011.369930  4413.143608  ...   \n",
       "..           ...          ...             ...           ...          ...  ...   \n",
       "223     0.000000     0.000000        0.000000      0.000000     0.000000  ...   \n",
       "224     0.000000     0.000000        0.000000      0.000000     0.000000  ...   \n",
       "225     0.000000     0.000000        0.000000      0.000000     0.000000  ...   \n",
       "226     0.000000     0.000000        0.000000      0.000000     0.000000  ...   \n",
       "227     0.000000     0.000000        0.000000      0.000000     0.000000  ...   \n",
       "\n",
       "     당근_가격(원/kg)  파프리카_가격(원/kg)  새송이_가격(원/kg)  팽이버섯_가격(원/kg)  토마토_가격(원/kg)  \\\n",
       "0    1543.306187    3657.336922   1653.562665    1558.366600   1978.029460   \n",
       "1    1677.199602    4038.487540   1978.420615    1855.822767   2814.846479   \n",
       "2    1933.192983    5436.511889   2598.682880    2344.333830   4154.447987   \n",
       "3    1511.206403    3355.762516   1605.403304    1519.446560   1881.702787   \n",
       "4    1654.286608    3715.861926   1980.861783    1870.338272   2700.905111   \n",
       "..           ...            ...           ...            ...           ...   \n",
       "223     0.000000       0.000000      0.000000       0.000000      0.000000   \n",
       "224     0.000000       0.000000      0.000000       0.000000      0.000000   \n",
       "225     0.000000       0.000000      0.000000       0.000000      0.000000   \n",
       "226     0.000000       0.000000      0.000000       0.000000      0.000000   \n",
       "227     0.000000       0.000000      0.000000       0.000000      0.000000   \n",
       "\n",
       "     청상추_가격(원/kg)  백다다기_가격(원/kg)  애호박_가격(원/kg)  캠벨얼리_가격(원/kg)  샤인마스캇_가격(원/kg)  \n",
       "0     2268.147852    1975.494076   1098.205279    2117.845975     7496.036291  \n",
       "1     3660.647615    2269.150782   1537.469433    2350.755967     3933.650255  \n",
       "2     5233.349069    2469.133577   2736.994448    3101.043589     1223.930791  \n",
       "3     1525.904181    1857.848692    907.637639    2023.351006     7934.085131  \n",
       "4     3171.180625    2171.252013   1438.130309    2161.055803     3901.496530  \n",
       "..            ...            ...           ...            ...             ...  \n",
       "223      0.000000       0.000000      0.000000       0.000000        0.000000  \n",
       "224      0.000000       0.000000      0.000000       0.000000        0.000000  \n",
       "225      0.000000       0.000000      0.000000       0.000000        0.000000  \n",
       "226      0.000000       0.000000      0.000000       0.000000        0.000000  \n",
       "227      0.000000       0.000000      0.000000       0.000000        0.000000  \n",
       "\n",
       "[228 rows x 22 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"Informer_price_volume.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
